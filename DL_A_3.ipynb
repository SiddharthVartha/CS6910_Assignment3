{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1qnQorOWhwyf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "outputId": "f63f5189-1367-491c-bad7-7aa9d54d28db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.1.1-py2.py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.3/277.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.1.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "wandb.login()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MNknYo2IfkCc"
      },
      "outputs": [],
      "source": [
        "def read_and_split_data(train_file, test_file, val_file):\n",
        "    # Read data\n",
        "    train_data = pd.read_csv(train_file, header=None)\n",
        "    test_data = pd.read_csv(test_file, header=None)\n",
        "    val_data = pd.read_csv(val_file, header=None)\n",
        "\n",
        "    # Split into English and Marathi words\n",
        "    english_train = train_data.iloc[:, 0]\n",
        "    marathi_train = train_data.iloc[:, 1]\n",
        "\n",
        "    english_test = test_data.iloc[:, 0]\n",
        "    marathi_test = test_data.iloc[:, 1]\n",
        "\n",
        "    english_val = val_data.iloc[:, 0]\n",
        "    marathi_val = val_data.iloc[:, 1]\n",
        "\n",
        "    return (english_train, marathi_train, english_test, marathi_test, english_val, marathi_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "m6z-uILhfJ5h"
      },
      "outputs": [],
      "source": [
        "# File paths\n",
        "lang=\"mar\"\n",
        "train_file = f\"/content/drive/MyDrive/DeepLearning/aksharantar_sampled/{lang}/{lang}_train.csv\"\n",
        "test_file = f\"/content/drive/MyDrive/DeepLearning/aksharantar_sampled/{lang}/{lang}_test.csv\"\n",
        "val_file = f\"/content/drive/MyDrive/DeepLearning/aksharantar_sampled/{lang}/{lang}_valid.csv\"\n",
        "\n",
        "# Call the function\n",
        "english_train, marathi_train, english_test, marathi_test, english_val, marathi_val = read_and_split_data(train_file, test_file, val_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RbF5_Ta7grGl"
      },
      "outputs": [],
      "source": [
        "def create_char_list(words):\n",
        "    char_set = set(char for word in words for char in word)\n",
        "    char_list = sorted(char_set)\n",
        "    max_length_word = max(len(word) for word in words)\n",
        "    return char_list, max_length_word\n",
        "\n",
        "\n",
        "def find_max_length(word_list):\n",
        "    max_length = -1\n",
        "    for word in word_list:\n",
        "        max_length = max(max_length, len(word))\n",
        "    return max_length\n",
        "\n",
        "# Create character lists and find maximum word lengths\n",
        "english_chars, english_max_len = create_char_list(english_train)\n",
        "marathi_chars, marathi_max_len = create_char_list(marathi_train)\n",
        "\n",
        "# Find maximum word lengths from validation and test data\n",
        "english_max_len = max(find_max_length(english_val), find_max_length(english_test), english_max_len)\n",
        "marathi_max_len = max(find_max_length(marathi_val), find_max_length(marathi_test), marathi_max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "FL9AwUVghDu-"
      },
      "outputs": [],
      "source": [
        "def word_to_vector(word, lang):\n",
        "    max_len = -1\n",
        "    if lang == \"english\":\n",
        "        max_len = english_max_len\n",
        "    else:\n",
        "        max_len = marathi_max_len\n",
        "\n",
        "    vector = [0] * (max_len + 2)  # Initialize vector with max length + 2 (for special tokens)\n",
        "    vector[0] = len(english_chars) + 1 if lang == \"english\" else len(marathi_chars) + 1\n",
        "    count=1\n",
        "    if(lang == \"english\"):\n",
        "        for char in word:\n",
        "            for i in range(len(english_chars)):\n",
        "                if(english_chars[i] == char):\n",
        "                    vector[count]=i+1\n",
        "                    count+=1\n",
        "    else :\n",
        "        for char in word:\n",
        "            for i in range(len(marathi_chars)):\n",
        "                if(marathi_chars[i] == char):\n",
        "                    vector[count]=i+1\n",
        "                    count+=1\n",
        "\n",
        "    return vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xeJ3-BojJXL",
        "outputId": "739cb476-1d8a-4757-b249-be44226f4fce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "रुद्रांची\n"
          ]
        }
      ],
      "source": [
        "print(marathi_train[13])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "qigC8n59hMVu"
      },
      "outputs": [],
      "source": [
        "vec = word_to_vector(marathi_train[13],\"marathi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaAzzdb-kriV",
        "outputId": "233ff000-ea1d-4880-e1fb-c5810efe6929"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[64, 42, 54, 33, 63, 42, 51, 2, 21, 53, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "25npFzyMlN9F"
      },
      "outputs": [],
      "source": [
        "# creating matrix of representation\n",
        "def word_matrix(words, language):\n",
        "    matrix = []\n",
        "    for word in words:\n",
        "        matrix.append(word_to_vector(word, language))\n",
        "    return torch.tensor(matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "H_a5L2zIltAN"
      },
      "outputs": [],
      "source": [
        "def prepare_word_matrices(train_data, val_data, test_data, language):\n",
        "    train_matrix = word_matrix(train_data, language)\n",
        "    val_matrix = word_matrix(val_data, language)\n",
        "    test_matrix = word_matrix(test_data, language)\n",
        "    return train_matrix, val_matrix, test_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8ldk0JNTl24R"
      },
      "outputs": [],
      "source": [
        "english_matrix, english_matrix_val, english_matrix_test = prepare_word_matrices(english_train, english_val, english_test, \"english\")\n",
        "marathi_matrix, marathi_matrix_val, marathi_matrix_test = prepare_word_matrices(marathi_train, marathi_val, marathi_test, \"marathi\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_dim, hidden_size, num_layers, batch_size, dropout_prob, bidirectional, cell_type):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_size = batch_size\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.biderectional = bidirectional\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "        self.cell_type = cell_type\n",
        "\n",
        "        rnn_class = nn.RNN if cell_type == \"RNN\" else (nn.LSTM if cell_type == \"LSTM\" else nn.GRU)\n",
        "        self.rnn = rnn_class(embedding_dim, hidden_size, num_layers, dropout=dropout_prob, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            output, (hidden, cell) = self.rnn(embedded)\n",
        "        else:\n",
        "            output, hidden = self.rnn(embedded)\n",
        "\n",
        "        return (output, hidden, cell) if self.cell_type == \"LSTM\" else (output, hidden)\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "p-DiYudYZwAF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, output_size, dec_layers, p, cell_type, attention=False, bidirectional=False):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dec_layers = dec_layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.cell_type = cell_type\n",
        "        self.attention = attention\n",
        "        self.bidirectional = bidirectional\n",
        "        self.max_length = len(english_matrix[0])\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        rnn_input_size = hidden_size if attention else embedding_size\n",
        "        rnn_class = nn.RNN if cell_type == \"RNN\" else (nn.LSTM if cell_type == \"LSTM\" else nn.GRU)\n",
        "        self.rnn = rnn_class(rnn_input_size, hidden_size, dec_layers, dropout=p)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        if attention:\n",
        "            self.attn = nn.Linear(hidden_size + embedding_size, self.max_length)\n",
        "            self.attn_combine = nn.Linear(hidden_size * 2 + embedding_size, hidden_size) if bidirectional else nn.Linear(hidden_size + embedding_size, hidden_size)\n",
        "\n",
        "    def forward(self, x, output, hidden, cell=None):\n",
        "        x = x.unsqueeze(0)\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        if self.attention:\n",
        "            attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "            attn_applied = torch.bmm(attn_weights.unsqueeze(1), output.permute(1, 0, 2)).squeeze(1)\n",
        "            op = torch.cat((embedded[0], attn_applied), 1)\n",
        "            op = self.attn_combine(op).unsqueeze(0)\n",
        "            op = F.relu(op)\n",
        "        else:\n",
        "            op = embedded\n",
        "\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            outputs, (hidden, cell) = self.rnn(op, (hidden, cell))\n",
        "        else:\n",
        "            outputs, hidden = self.rnn(op, hidden)\n",
        "\n",
        "        predictions, hidden, cell = self.generate_predictions(outputs, hidden, cell)\n",
        "\n",
        "        return predictions, hidden, cell\n",
        "\n",
        "    def generate_predictions(self, rnn_outputs, rnn_hidden, rnn_cell=None):\n",
        "        output_predictions = self.fc(rnn_outputs)\n",
        "        output_predictions = output_predictions.squeeze(0)\n",
        "\n",
        "        return (output_predictions, rnn_hidden, rnn_cell) if self.cell_type == \"LSTM\" else (output_predictions, rnn_hidden ,rnn_cell)\n",
        "\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "metadata": {
        "id": "kU8laIvE1C_v"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqModel(nn.Module):\n",
        "    def __init__(self, output_size, cell_type, bidirectional, enc_layers, dec_layers, encoder, decoder):\n",
        "        super(Seq2SeqModel, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.cell_type = cell_type\n",
        "        self.bidirectional = bidirectional\n",
        "        self.enc_layers = enc_layers\n",
        "        self.dec_layers = dec_layers\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, source, target , teacher_force_ratio=0.5):\n",
        "        target_len = target.shape[0]\n",
        "        batch_size = source.shape[1]\n",
        "\n",
        "        outputs = torch.zeros(target_len, batch_size, self.output_size).to(source.device)\n",
        "\n",
        "        encoder_output, hidden, cell = self.encode_sequence(source)\n",
        "        hidden, cell = self.prepare_decoder_states(hidden, cell)\n",
        "        outputs = self.decode_sequence(target, encoder_output, hidden, cell, teacher_force_ratio)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def encode_sequence(self, source):\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            encoder_output, hidden, cell = self.encoder(source)\n",
        "            return encoder_output, hidden, cell\n",
        "        else:\n",
        "            encoder_output, hidden = self.encoder(source)\n",
        "            return encoder_output, hidden, None\n",
        "\n",
        "    def prepare_decoder_states(self, hidden, cell):\n",
        "        if self.bidirectional or self.enc_layers != self.dec_layers:\n",
        "          hidden = hidden[self.enc_layers - 1] + hidden[self.enc_layers - 1]\n",
        "          hidden = hidden.repeat(self.dec_layers,1,1)\n",
        "          if(self.cell_type == \"LSTM\"):\n",
        "              cell = cell[self.enc_layers - 1] + cell[self.enc_layers - 1]\n",
        "              cell = cell.repeat(self.dec_layers,1,1)\n",
        "        return hidden, cell\n",
        "\n",
        "\n",
        "    def decode_sequence(self, tgt, enc_out, hid, cell, teacher_force_ratio):\n",
        "        batch_size = tgt.shape[1]\n",
        "        target_len = tgt.shape[0]\n",
        "        outputs = torch.zeros(target_len, batch_size, self.output_size).to(enc_out.device)\n",
        "\n",
        "        timestep = 1\n",
        "        current_token = tgt[0]\n",
        "\n",
        "        while timestep < target_len:\n",
        "            if self.cell_type == \"LSTM\":\n",
        "                output, hid, cell = self.decoder(current_token, enc_out, hid, cell)\n",
        "            else:\n",
        "                output, hid, cell = self.decoder(current_token, enc_out, hid)\n",
        "            outputs[timestep] = output\n",
        "\n",
        "            if random.random() < teacher_force_ratio:\n",
        "                current_token = tgt[timestep] if timestep < target_len - 1 else output.argmax(1)\n",
        "            else:\n",
        "                current_token = output.argmax(1)\n",
        "\n",
        "            timestep += 1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "5xY9str_1F2E"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, input_data, target_data, batch_size):\n",
        "    correct_count = 0\n",
        "    total_samples = len(input_data)\n",
        "    for idx in range(0, total_samples, batch_size):\n",
        "        input_batch = input_data[idx:idx + batch_size].to(device)\n",
        "        target_batch = target_data[idx:idx + batch_size].to(device)\n",
        "\n",
        "        output = model(input_batch.T, target_batch.T, teacher_force_ratio=0)\n",
        "        predicted_tokens = torch.argmax(F.softmax(output, dim=2), dim=2).T\n",
        "\n",
        "        correct_count += torch.all(predicted_tokens[:, 1:] == target_batch[:, 1:], dim=1).sum().item()\n",
        "\n",
        "    accuracy = correct_count * 100 / total_samples\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "WBIL7mwf1TVq"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(epochs, learning_rate, cell_type, bidirectional, enc_layers, dec_layers, batch_size, embedding_dim, hidden_size, enc_dropout, dec_dropout,attention):\n",
        "    pad_idx = len(marathi_chars) + 1\n",
        "\n",
        "    input_size_encoder = len(english_chars)\n",
        "    input_size_decoder = len(marathi_chars)\n",
        "    output_size = len(marathi_chars)\n",
        "    input_size_encoder+=2\n",
        "    input_size_decoder+=2\n",
        "    output_size+=2\n",
        "\n",
        "    encoder = Encoder(input_size_encoder, embedding_dim, hidden_size, enc_layers,batch_size, enc_dropout,bidirectional, cell_type).to(device)\n",
        "    decoder= Decoder(input_size_decoder,embedding_dim,hidden_size,output_size,dec_layers,dec_dropout, cell_type,attention,bidirectional).to(device)\n",
        "\n",
        "    model = Seq2SeqModel(output_size, cell_type, bidirectional, enc_layers, dec_layers ,encoder, decoder).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch: \", epoch+1)\n",
        "\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        val_loss = 0\n",
        "        step = 0\n",
        "        total_batches = len(english_matrix) // batch_size\n",
        "\n",
        "        for batch_idx in tqdm(range(total_batches)):\n",
        "            start_idx = batch_size * batch_idx\n",
        "            end_idx = batch_size * (batch_idx + 1)\n",
        "\n",
        "            inp_data = english_matrix[start_idx:end_idx].to(device)\n",
        "            target = marathi_matrix[start_idx:end_idx].to(device)\n",
        "            target = target.T\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(inp_data.T, target)\n",
        "\n",
        "            output = output[1:].reshape(-1, output.shape[2])\n",
        "            target = target[1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_batches = len(english_matrix_val) // batch_size\n",
        "            for val_batch_idx in range(val_batches):\n",
        "                val_start_idx = batch_size * val_batch_idx\n",
        "                val_end_idx = batch_size * (val_batch_idx + 1)\n",
        "\n",
        "                val_inp_data = english_matrix_val[val_start_idx:val_end_idx].to(device)\n",
        "                val_target = marathi_matrix_val[val_start_idx:val_end_idx].to(device)\n",
        "                val_target = val_target.T\n",
        "\n",
        "                val_output = model(val_inp_data.T, val_target)\n",
        "                val_output = val_output[1:].reshape(-1, val_output.shape[2])\n",
        "                val_target = val_target[1:].reshape(-1)\n",
        "\n",
        "                val_loss += criterion(val_output, val_target).item()\n",
        "\n",
        "            val_loss /= val_batches\n",
        "        training_accuracy = calculate_accuracy(model, english_matrix, marathi_matrix, batch_size)\n",
        "        val_accuracy = calculate_accuracy(model, english_matrix_val, marathi_matrix_val, batch_size)\n",
        "        wandb.log({\n",
        "            \"Epoch\": epoch+1,\n",
        "            \"Loss\": total_loss / step,\n",
        "            \"Accuracy\": training_accuracy,\n",
        "            \"Val_Accuracy\": val_accuracy,\n",
        "            \"Val_Loss\": val_loss\n",
        "        })\n",
        "        print(f\"Loss: {total_loss/step}\\t Accuracy: {training_accuracy}\\t Val_Accuracy: {val_accuracy}\\t Val_Loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "hx8anR1Q1Kai"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sweep configuration\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    'metric': {\n",
        "        'name': 'Val_Accuracy',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    \"parameters\": {\n",
        "        \"epochs\": {\"values\": [ 10, 15 , 20]},  # Define the hyperparameter search space\n",
        "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
        "        \"cell_type\": {\"values\": [\"RNN\",\"LSTM\", \"GRU\"]},\n",
        "        \"bidirectional\": {\"values\": [True, False]},\n",
        "        \"enc_layers\": {\"values\": [1, 2, 3, 4 ,5]},\n",
        "        \"dec_layers\": {\"values\": [1, 2, 3, 4 ,5]},\n",
        "        \"batch_size\": {\"values\": [128, 256, 512]},\n",
        "        \"embedding_dim\": {\"values\": [256, 384, 512]},\n",
        "        \"hidden_size\": {\"values\": [256, 384, 512]},\n",
        "        \"enc_dropout\": {\"values\": [0, 0.1, 0.2]},\n",
        "        \"dec_dropout\": {\"values\": [0, 0.1, 0.2]},\n",
        "        \"attention\": {\"values\": [False]}\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "7dg_IgyKFDYc"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Initialize wandb\n",
        "    wandb.init()\n",
        "    config = wandb.config\n",
        "    wandb.run.name = \"_\".join([f\"{param}:{value}\" for param, value in config.items()])\n",
        "    train_model(**config)\n",
        "\n",
        "# Initialize the sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"deep_learn_assignment_3\",entity=\"cs23m063\")\n",
        "\n",
        "# Run the sweep\n",
        "wandb.agent(sweep_id, function=main,count=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "lkvArq36FJUJ",
        "outputId": "0aa9c6c2-7c8a-4596-9996-876d2cbd22dd"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: csa50sy3\n",
            "Sweep URL: https://wandb.ai/cs23m063/deep_learn_assignment_3/sweeps/csa50sy3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2e4jbce2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_dropout: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_dim: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_dropout: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m063\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240512_183608-2e4jbce2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs23m063/deep_learn_assignment_3/runs/2e4jbce2' target=\"_blank\">hardy-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m063/deep_learn_assignment_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m063/deep_learn_assignment_3/sweeps/csa50sy3' target=\"_blank\">https://wandb.ai/cs23m063/deep_learn_assignment_3/sweeps/csa50sy3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/cs23m063/deep_learn_assignment_3' target=\"_blank\">https://wandb.ai/cs23m063/deep_learn_assignment_3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs23m063/deep_learn_assignment_3/sweeps/csa50sy3' target=\"_blank\">https://wandb.ai/cs23m063/deep_learn_assignment_3/sweeps/csa50sy3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/cs23m063/deep_learn_assignment_3/runs/2e4jbce2' target=\"_blank\">https://wandb.ai/cs23m063/deep_learn_assignment_3/runs/2e4jbce2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 4/200 [00:27<22:29,  6.89s/it]\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hHOsBfjC1dAR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1LgxO2cJI45RCxPsXLWbREo0DatxfZuGr",
      "authorship_tag": "ABX9TyMoFhaYmGTXhoBZ/yjifXBp"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}